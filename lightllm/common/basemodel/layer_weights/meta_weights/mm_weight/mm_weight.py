import os
import flux
import torch
from torch.distributed import ProcessGroup
from abc import abstractmethod
from typing import Optional, Tuple, List, Dict, Union
from lightllm.common.basemodel.layer_infer.cache_tensor_manager import g_cache_manager
from lightllm.common.quantization.quantize_method import QuantizationMethod
from lightllm.common.basemodel.layer_weights.meta_weights.base_weight import BaseWeightTpl
from lightllm.common.quantization import Quantcfg
from lightllm.utils.dist_utils import get_current_device_id
from lightllm.utils.log_utils import init_logger

logger = init_logger(__name__)


def generate_scale_name(name, weight_scale_suffix, act_scale_suffix):
    weight_scale_name = None
    act_scale_name = None
    if weight_scale_suffix is not None:
        weight_scale_name = ".".join(name.split(".")[:-1] + [weight_scale_suffix])
    if act_scale_suffix is not None:
        act_scale_name = ".".join(name.split(".")[:-1] + [act_scale_suffix])
    return weight_scale_name, act_scale_name


class MMWeightTpl(BaseWeightTpl):
    def __init__(
        self,
        data_type: torch.dtype,
        quant_method: QuantizationMethod = None,
        tp_rank: int = None,
        tp_world_size: int = None,
    ) -> None:
        super().__init__(tp_rank, tp_world_size, data_type)
        self.quant_method = quant_method
        self.weight: Optional[torch.Tensor] = None
        self.bias: Optional[torch.Tensor] = None
        # quantized_weight 用于标记加载的权重是已经量化的权重格式
        # 不需要做在线量化
        self.quantized_weight: bool = False
        # 标记是否存在 bias， 由子类初始化
        self.has_bias: bool = None

    def flux_gemm_rs(
        self,
        input_tensor: torch.Tensor,
        out: Optional[torch.Tensor] = None,
        use_custom_tensor_mananger: bool = True,
        transpose_weight: bool = True,
    ):
        # logger.debug("flux_gemm_rs kernel start")
        # check the weight contiguous
        if not self.weight.is_contiguous():
            self.weight = self.weight.contiguous()
        assert self.weight.is_contiguous(), "weight must be contiguous"

        local_M = input_tensor.size(0)
        M = local_M * self.tp_world_size_
        N = self.weight.size(1)

        if out is None:
            # out shape: [4, 4096]
            shape = (local_M, self.weight.shape[1])
            dtype = input_tensor.dtype
            device = input_tensor.device
            if use_custom_tensor_mananger:
                out = g_cache_manager.alloc_tensor(shape, dtype, device=device, is_graph_out=False)
            else:
                out = torch.zeros(shape, dtype=dtype, device=device)
        # logger.debug(f"{input_tensor.shape}, {self.weight.shape}, {out.shape}")

        # with flux.util.group_profile(
        #     name="gemm_rs_" + os.environ["TORCHELASTIC_RUN_ID"], do_prof=False, group=torch.distributed.group.WORLD
        # ):
        gemm_rs_op = flux.GemmRS(
            torch.distributed.group.WORLD,
            1,
            (M + 1024 - 1) // 1024 * 1024,
            N,
            input_tensor.dtype,
            out.dtype,
            transpose_weight=transpose_weight,
        )
        # logger.debug(f"gemm_rs_kernel initialized M={M}, N={N}, local_M={local_M}")
        out = gemm_rs_op.forward(
            input_tensor,
            self.weight,
            bias=self.bias,
            fast_accum=False,
            reduce_scatter_option=flux.ReduceScatterOption(),
        )
        return out

    def flux_ag_gemm(
        self,
        input_tensor: torch.Tensor,
        out: Optional[torch.Tensor] = None,
        use_custom_tensor_mananger: bool = True,
        transpose_weight: bool = True,
    ):
        # logger.debug("flux_ag_gemm kernel start")
        # check the weight contiguous
        if not self.weight.is_contiguous():
            self.weight = self.weight.contiguous()
        assert self.weight.is_contiguous(), "weight must be contiguous"

        local_M = input_tensor.size(0)
        M = local_M * self.tp_world_size_
        K = input_tensor.size(1)
        N = self.weight.size(1)

        if out is None:
            shape = (M, self.weight.shape[1])
            dtype = input_tensor.dtype
            device = input_tensor.device
            if use_custom_tensor_mananger:
                out = g_cache_manager.alloc_tensor(shape, dtype, device=device, is_graph_out=False)
            else:
                out = torch.zeros(shape, dtype=dtype, device=device)
        # logger.debug(f"{input_tensor.shape}, {self.weight.shape}, {out.shape}")

        # with flux.util.group_profile(
        #     name="ag_gemm_" + os.environ["TORCHELASTIC_RUN_ID"], do_prof=False, group=torch.distributed.group.WORLD
        # ):
        ag_option = flux.AllGatherOption()
        ag_gemm_op = flux.AGKernel(
            torch.distributed.group.WORLD,
            1,
            M,
            N,
            K,
            input_tensor.dtype,
            output_dtype=out.dtype,
        )
        # full_input = torch.empty((M, K), dtype=input_tensor.dtype, device=input_tensor.device)
        # logger.debug(f"ag_gemm_kernel initialized M={M}, N={N}, K={K}")
        out = ag_gemm_op.forward(
            input_tensor,
            self.weight,
            output=out,
            bias=self.bias,
            transpose_weight=transpose_weight,
            # gathered_input=full_input,
            all_gather_option=ag_option,
        )
        return out, None

    def mm(
        self, input_tensor: torch.Tensor, out: Optional[torch.Tensor] = None, use_custom_tensor_mananger: bool = True
    ) -> torch.Tensor:
        if self.quant_method is not None:
            return self.quant_method.apply(
                input_tensor, self.weight, self.bias, out, use_custom_tensor_mananger=use_custom_tensor_mananger
            )
        if out is None:
            shape = (input_tensor.shape[0], self.weight.shape[1])
            dtype = input_tensor.dtype
            device = input_tensor.device
            if use_custom_tensor_mananger:
                out = g_cache_manager.alloc_tensor(shape, dtype, device=device, is_graph_out=False)
            else:
                out = torch.empty(shape, dtype=dtype, device=device)
        if self.bias is None:
            return torch.mm(input_tensor, self.weight, out=out)
        return torch.addmm(self.bias, input_tensor, self.weight, out=out)

    def verify_load(self) -> bool:
        load_ok = True
        # Verify weight. The weight must be not None.
        load_ok = load_ok and self.weight is not None
        # Verify bias. If bias_name is set, it must be not None.
        if self.has_bias:
            load_ok = load_ok and self.bias is not None
        return load_ok

    def _process_weight(self, weight) -> None:
        if self.quant_method is not None and not self.quantized_weight:
            self.weight = self.quant_method.quantize(weight.to(self.data_type_).cuda(get_current_device_id()))
            return
        # 让 k dim 更连续，大多数split k 算法的算子可能能更快
        self.weight = weight.cuda(get_current_device_id()).transpose(0, 1)

    def _load_weights(self, weights: Dict[str, torch.Tensor]) -> None:
        if self.weight_name in weights:
            weight = self._slice_weight(weights[self.weight_name])
            self._process_weight(weight)

        if self.bias_name in weights:
            self.bias = self._slice_bias(weights[self.bias_name]).cuda(get_current_device_id())
        return


class MultiMMWeightTpl(MMWeightTpl):
    def __init__(
        self,
        weight_names: str,
        data_type: torch.dtype,
        bias_names: Optional[str] = None,
        quant_method: QuantizationMethod = None,
        tp_rank: int = None,
        tp_world_size: int = None,
    ) -> None:
        super().__init__(data_type, quant_method, tp_rank, tp_world_size)

        self.weight_names = weight_names
        self.bias_names = bias_names
        self.weights = [None] * len(self.weight_names)
        if self.bias_names is not None:
            self.biases = [None] * len(self.bias_names)
            self.has_bias = all(b is not None for b in self.bias_names) and len(bias_names) > 0
        else:
            self.biases = None
            self.has_bias = False

    def _pre_porcess_weights(self, weights: Dict[str, torch.Tensor]) -> None:
        for i in range(len(self.weight_names)):
            if self.weight_names[i] in weights:
                weight = weights[self.weight_names[i]]
                self.weights[i] = self._slice_weight(weight)
            if self.has_bias and self.bias_names[i] in weights:
                bias = weights[self.bias_names[i]]
                self.biases[i] = self._slice_bias(bias)

    def _fuse_weights(self) -> None:
        if self.weight is None and (None not in self.weights):
            weight = torch.cat(self.weights, dim=0)
            self._process_weight(weight)
            delattr(self, "weights")

        if self.has_bias and self.bias is None and (None not in self.biases):
            self.bias = torch.cat(self.biases, dim=0).cuda(get_current_device_id())
            delattr(self, "biases")
        return self

    def _load_weights(self, weights: Dict[str, torch.Tensor]) -> None:
        self._pre_porcess_weights(weights)

    def load_hf_weights(self, weights):
        super().load_hf_weights(weights)
        self._fuse_weights()
        return


class BMMWeightTpl(MMWeightTpl):
    def mm(
        self, input_tensor: torch.Tensor, out: Optional[torch.Tensor] = None, use_custom_tensor_mananger: bool = True
    ) -> torch.Tensor:
        raise RuntimeError("use bmm not mm")

    def bmm(
        self, input_tensor: torch.Tensor, out: Optional[torch.Tensor] = None, use_custom_tensor_mananger: bool = True
    ) -> torch.Tensor:
        # 目前 bmm 不支持量化运算操作
        fpweight = self.weight
        if out is None:
            shape = (input_tensor.shape[0], input_tensor.shape[1], fpweight.shape[2])
            dtype = input_tensor.dtype
            device = input_tensor.device
            if use_custom_tensor_mananger:
                out = g_cache_manager.alloc_tensor(shape, dtype, device=device, is_graph_out=False)
            else:
                out = torch.empty(shape, dtype=dtype, device=device)
        if self.bias is None:
            return torch.bmm(input_tensor, fpweight, out=out)
        return torch.addbmm(self.bias, input_tensor, fpweight, out=out)

    def _process_weight(self, weight) -> None:
        self.weight = weight.cuda(get_current_device_id())


class MMWeight:
    def __new__(cls, **kwargs):
        quant_cfg = kwargs.pop("quant_cfg", None)
        layer_num_ = kwargs.pop("layer_num", None)
        name = kwargs.pop("name", None)
        quant_method, quantized_weight = cls._get_quant_method(quant_cfg, layer_num_, name)
        kwargs["quant_method"] = quant_method
        mmcls = cls._get_mmcls(quant_method, quantized_weight)
        return mmcls(**kwargs)

    @classmethod
    def _get_quant_method(cls, quant_cfg: Quantcfg, layer_num_: int, name: str) -> QuantizationMethod:
        if quant_cfg is None:
            return None, False
        quant_method = quant_cfg.get_quant_method(layer_num_, name)
        quant_type = quant_cfg.get_quant_type(layer_num_, name)
        quantized_weight = quant_cfg.quantized_weight
        if quant_method is not None:
            logger.info(f"Layer {layer_num_} {name} is set to {quant_type}")
        return quant_method, quantized_weight

    @classmethod
    def _get_mmcls(
        cls, quant_method: QuantizationMethod
    ) -> Optional[Union[MMWeightTpl, MultiMMWeightTpl, BMMWeightTpl]]:
        return None

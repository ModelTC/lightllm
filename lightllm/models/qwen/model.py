import os
import json
import torch

from .layer_infer.transformer_layer_inference import QwenTransformerLayerInfer
from .layer_weights.pre_and_post_layer_weight import QwenPreAndPostLayerWeight
from .layer_weights.transformer_layer_weight import QwenTransformerLayerWeight

from lightllm.models.llama.model import LlamaTpPartModel
from lightllm.common.build_utils import repair_config


class QWenTpPartModel(LlamaTpPartModel):
    # weight class
    pre_and_post_weight_class = QwenPreAndPostLayerWeight
    transformer_weight_class = QwenTransformerLayerWeight

    # infer class
    transformer_layer_infer_class = QwenTransformerLayerInfer

    # infer state class
    def __init__(self, tp_rank, world_size, weight_dir, max_total_token_num, load_way="HF", mode=""):
        super().__init__(tp_rank, world_size, weight_dir, max_total_token_num, load_way, mode)
    

    def _init_config(self):
        super()._init_config()
        # rename key
        # repair_config()
        repair_config(self.config, same_names=["ffn_hidden_size", "intermediate_size"])
        repair_config(self.config, same_names=["rms_norm_eps", "layer_norm_epsilon"])
        return 
    
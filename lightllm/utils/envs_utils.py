import os
import json
import torch
from easydict import EasyDict
from functools import lru_cache
from lightllm.utils.log_utils import init_logger


logger = init_logger(__name__)


def set_unique_server_name(args):
    if args.run_mode == "pd_master":
        os.environ["LIGHTLLM_UNIQUE_SERVICE_NAME_ID"] = str(args.port) + "_pd_master"
    else:
        os.environ["LIGHTLLM_UNIQUE_SERVICE_NAME_ID"] = str(args.nccl_port) + "_" + str(args.node_rank)
    return


@lru_cache(maxsize=None)
def get_unique_server_name():
    service_uni_name = os.getenv("LIGHTLLM_UNIQUE_SERVICE_NAME_ID")
    return service_uni_name


def set_cuda_arch(args):
    if not torch.cuda.is_available():
        return
    if args.enable_flashinfer_prefill or args.enable_flashinfer_decode:
        capability = torch.cuda.get_device_capability()
        arch = f"{capability[0]}.{capability[1]}"
        os.environ["TORCH_CUDA_ARCH_LIST"] = f"{arch}{'+PTX' if arch == '9.0' else ''}"


def set_env_start_args(args):
    set_cuda_arch(args)
    if not isinstance(args, dict):
        args = vars(args)
    os.environ["LIGHTLLM_START_ARGS"] = json.dumps(args)
    return


@lru_cache(maxsize=None)
def get_env_start_args():
    from lightllm.server.core.objs.start_args_type import StartArgs

    start_args: StartArgs = json.loads(os.environ["LIGHTLLM_START_ARGS"])
    start_args: StartArgs = EasyDict(start_args)
    return start_args


@lru_cache(maxsize=None)
def enable_env_vars(args):
    return os.getenv(args, "False").upper() in ["ON", "TRUE", "1"]


@lru_cache(maxsize=None)
def get_deepep_num_max_dispatch_tokens_per_rank():
    # 该参数需要大于单卡最大batch size，且是8的倍数。该参数与显存占用直接相关，值越大，显存占用越大，如果出现显存不足，可以尝试调小该值
    return int(os.getenv("NUM_MAX_DISPATCH_TOKENS_PER_RANK", 256))


def get_lightllm_gunicorn_time_out_seconds():
    return int(os.getenv("LIGHTLMM_GUNICORN_TIME_OUT", 180))


def get_lightllm_gunicorn_keep_alive():
    return int(os.getenv("LIGHTLMM_GUNICORN_KEEP_ALIVE", 10))


@lru_cache(maxsize=None)
def get_lightllm_websocket_max_message_size():
    """
    Get the maximum size of the WebSocket message.
    :return: Maximum size in bytes.
    """
    return int(os.getenv("LIGHTLLM_WEBSOCKET_MAX_SIZE", 16 * 1024 * 1024))


# get_redundancy_expert_ids 和 get_redundancy_expert_num 主要是用于推理时的冗余专家的id和数量的获取
# 其依赖的配置文件是 ep_redundancy_expert_config_path 是一个json格式的文本文件，其内容格式如下:
# {
#   "redundancy_expert_num": 1,
#   "0": [0],
#   "1": [0],
#   "default": [0,]
# }


@lru_cache(maxsize=None)
def get_redundancy_expert_ids(layer_index: int):
    """
    Get the redundancy expert ids from the environment variable.
    :return: List of redundancy expert ids.
    """
    args = get_env_start_args()
    if args.ep_redundancy_expert_config_path is None:
        return []

    with open(args.ep_redundancy_expert_config_path, "r") as f:
        config = json.load(f)
    if str(layer_index) in config:
        return config[str(layer_index)]
    else:
        return config.get("default", [])


@lru_cache(maxsize=None)
def get_redundancy_expert_num():
    """
    Get the number of redundancy experts from the environment variable.
    :return: Number of redundancy experts.
    """
    args = get_env_start_args()
    if args.ep_redundancy_expert_config_path is None:
        return 0

    with open(args.ep_redundancy_expert_config_path, "r") as f:
        config = json.load(f)
    if "redundancy_expert_num" in config:
        return config["redundancy_expert_num"]
    else:
        return 0


@lru_cache(maxsize=None)
def get_redundancy_expert_update_interval():
    return int(os.getenv("LIGHTLLM_REDUNDANCY_EXPERT_UPDATE_INTERVAL", 30 * 60))


@lru_cache(maxsize=None)
def get_redundancy_expert_update_max_load_count():
    return int(os.getenv("LIGHTLLM_REDUNDANCY_EXPERT_UPDATE_MAX_LOAD_COUNT", 1))
